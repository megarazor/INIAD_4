{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample program for calculating accuracy measures and plotting ROC / PR curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_in = 'class_pre1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_in, delimiter=',', skiprows=0, header=0)\n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusing matrix (混同行列) and measurements for prediction accuracy for each method  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A')\n",
    "print('precision:', precision_score(df['label'], df['A']>=0.5))\n",
    "print('recall:', recall_score(df['label'], df['A']>=0.5))\n",
    "print('accuracy(Q2):', accuracy_score(df['label'], df['A']>=0.5))\n",
    "print('f1:', f1_score(df['label'], df['A']>=0.5))\n",
    "print('matthews:', matthews_corrcoef(df['label'], df['A']>=0.5))\n",
    "#predA = (df['A']>=0.5).astype('int')\n",
    "predA = df['A'].map(lambda x: 1 if x >= 0.5 else 0)\n",
    "ctA = pd.crosstab(df['label'], predA)\n",
    "fpA = ctA.loc[0,1]\n",
    "tnA = ctA.loc[0,0]\n",
    "print('false positive rate:', fpA/(fpA+tnA))\n",
    "display(ctA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('B')\n",
    "print('precision:', precision_score(df['label'], df['B']>=0.5))\n",
    "print('recall:', recall_score(df['label'], df['B']>=0.5))\n",
    "print('accuracy(Q2):', accuracy_score(df['label'], df['B']>=0.5))\n",
    "print('f1:', f1_score(df['label'], df['B']>=0.5))\n",
    "print('matthews:', matthews_corrcoef(df['label'], df['B']>=0.5))\n",
    "#predB = (df['B']>=0.5).astype('int')\n",
    "predB = df['B'].map(lambda x: 1 if x >= 0.5 else 0)\n",
    "ctB = pd.crosstab(df['label'], predB)\n",
    "fpB = ctB.loc[0,1]\n",
    "tnB = ctB.loc[0,0]\n",
    "print('false positive rate:', fpB/(fpB+tnB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('C')\n",
    "print('precision:', precision_score(df['label'], df['C']>=0.5))\n",
    "print('recall:', recall_score(df['label'], df['C']>=0.5))\n",
    "print('accuracy(Q2):', accuracy_score(df['label'], df['C']>=0.5))\n",
    "print('f1:', f1_score(df['label'], df['C']>=0.5))\n",
    "print('matthews:', matthews_corrcoef(df['label'], df['C']>=0.5))\n",
    "#predC = (df['C']>=0.5).astype('int')\n",
    "predC = df['C'].map(lambda x: 1 if x >= 0.5 else 0)\n",
    "ctC = pd.crosstab(df['label'], predC)\n",
    "fpC = ctC.loc[0,1]\n",
    "tnC = ctC.loc[0,0]\n",
    "print('false positive rate:', fpC/(fpC+tnC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve and its AUC for each method  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df['label']\n",
    "y_score = df['A']\n",
    "fprA, tprA, thresholdsA = roc_curve(y_true, y_score)\n",
    "print('AUC(A):', roc_auc_score(y_true, y_score))\n",
    "#print('AUC(A):', auc(fprA, tprA))  # from fpr and tpr\n",
    "y_score = df['B']\n",
    "fprB, tprB, thresholdsB = roc_curve(y_true, y_score)\n",
    "print('AUC(B):', roc_auc_score(y_true, y_score))\n",
    "#print('AUC(B):', auc(fprB, tprB))  # from fpr and tpr\n",
    "y_score = df['C']\n",
    "fprC, tprC, thresholdsC = roc_curve(y_true, y_score)\n",
    "print('AUC(C):', roc_auc_score(y_true, y_score))\n",
    "#print('AUC(C):', auc(fprC, tprC)) # from fpr and tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fprA, tprA, label='A')\n",
    "plt.plot(fprB, tprB, label='B')\n",
    "plt.plot(fprC, tprC, label='C')\n",
    "plt.plot([0,1],[0,1])\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PR curve and its AUC for each method  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df['label']\n",
    "y_scoreA = df['A']\n",
    "precisionA, recallA, thresholdsA = precision_recall_curve(y_true, y_scoreA)\n",
    "print('PR-AUC(A):', auc(recallA, precisionA))\n",
    "y_scoreB = df['B']\n",
    "precisionB, recallB, thresholdsB = precision_recall_curve(y_true, y_scoreB)\n",
    "print('PR-AUC(B):', auc(recallB, precisionB))\n",
    "y_scoreC = df['C']\n",
    "precisionC, recallC, thresholdsC = precision_recall_curve(y_true, y_scoreC)\n",
    "print('PR-AUC(C):', auc(recallC, precisionC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recallA, precisionA, label='A')\n",
    "plt.plot(recallB, precisionB, label='B')\n",
    "plt.plot(recallC, precisionC, label='C')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.title('PR curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Accuracy around high score predictions to see the meaning of PR curves)  \n",
    "accuracy: B > A > C (this corresponds to PR-AUC)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[['label','A']].sort_values(by='A', ascending=False).head())\n",
    "display(df[['label','B']].sort_values(by='B', ascending=False).head())\n",
    "display(df[['label','C']].sort_values(by='C', ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
